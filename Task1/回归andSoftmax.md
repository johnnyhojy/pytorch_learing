# ??????

### ?????
```python
num_inputs = 2
num_examples = 1000

true_w = [2, -3.4]
true_b = 4.2

features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)
import torch.utils.data as Data

batch_size = 10

# combine featues and labels of dataset
dataset = Data.TensorDataset(features, labels)

# put dataset into DataLoader
data_iter = Data.DataLoader(
    dataset=dataset,            # torch TensorDataset format
    batch_size=batch_size,      # mini batch size
    shuffle=True,               # whether shuffle the data or not
    num_workers=2,              # read data in multithreading
)
```

### ???????????

<img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=4137943918,3894617166&fm=173&app=49&f=JPEG?w=302&h=218&s=3EAA78235146DD4D5AD581DB000080B1#pic_cener" >

##### Hpothesis???
```python
class LinearNet(nn.Module):
    def __init__(self, n_feature):
        super(LinearNet, self).__init__()      # call father function to init 
        self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`

    def forward(self, x):
        y = self.linear(x)
        return y
    
net = LinearNet(num_inputs)
print(net)
```

##### parameters:??
```python
from torch.nn import init

init.normal_(net[0].weight, mean=0.0, std=0.01)
init.constant_(net[0].bias, val=0.0)  # or you can use `net[0].bias.data.fill_(0)` to modify it directly
```

##### Cost Function? ????
```python
loss = nn.MSELoss()    # nn built-in squared loss function
                       # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`
```

##### Goal?????
```python
import torch.optim as optim

optimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent function
print(optimizer)  # function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`
```

##### ?????
```python
num_epochs = 3
for epoch in range(1, num_epochs + 1):
    for X, y in data_iter:
        output = net(X)
        l = loss(output, y.view(-1, 1))
        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()
        l.backward()
        optimizer.step()
    print('epoch %d, loss: %f' % (epoch, l.item()))
```

# softmax?????

<img src="https://s2.ax1x.com/2019/12/29/ln6yzn.jpg#shadow" width="422" >

sigmoid ????? input ??? [0,1] ??????????????????????????[0,1]??????????? 1?? ?pi=1
???????????? Softmax ?????????????

<img src="https://www.zhihu.com/equation?tex=y_%7Bi%7D+%3D+%5Cfrac%7Be%5E%7Ba_i%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BC%7De%5E%7Ba_k%7D%7D+%5C+%5C+%5C+%5Cforall+i+%5Cin+1...C" width="211" >



### ???????

????$i$???????$\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}$ ????$y^{(i)}$???$i$????????????1????0?????????????????????$\boldsymbol{\hat y}^{(i)}$??????????????$\boldsymbol{y}^{(i)}$?

- ??????  

$$
\begin{aligned}Loss = |\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}|^2/2\end{aligned}
$$
  

??????????????????????????????????????????????????$y^{(i)}=3$????????$\hat{y}^{(i)}_3$????????$\hat{y}^{(i)}_1$?$\hat{y}^{(i)}_2$???????$\hat{y}^{(i)}_3$??0.6???????????????????????????????????$\hat y^{(i)}_1=\hat y^{(i)}_2=0.2$?$\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4$??????????????????????????

????????????????????????????????????????cross entropy????????????


$$
H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},
$$


??????$y_j^{(i)}$???$\boldsymbol y^{(i)}$??0?1?????????????$i$??????????????$y^{(i)}$??????????????$\boldsymbol y^{(i)}$????$y^{(i)}$???$y^{(i)}{y^{(i)}}$?1?????0???$H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{y^{(i)}}^{(i)}$???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

????????????$n$??????????? 
$$
\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),
$$


??$\boldsymbol{\Theta}$????????????????????????????????????$\ell(\boldsymbol{\Theta}) = -(1/n) \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}$?????????????????$\ell(\boldsymbol{\Theta})$??????$\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}$?????????????????????????????????????


## softmax ??
```python
def softmax(X):
    X_exp = X.exp()
    partition = X_exp.sum(dim=1, keepdim=True)
    # print("X size is ", X_exp.size())
    # print("partition size is ", partition, partition.size())
    return X_exp / partition  # ?????????
```